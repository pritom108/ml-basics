{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5105722",
   "metadata": {},
   "source": [
    "# üß† Classification Algorithms ‚Äî Complete Guide\n",
    "\n",
    "Classification algorithms are **supervised learning models** that predict **discrete class labels**, such as Yes/No, Spam/Not Spam, Cat/Dog, etc.\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£ What is Classification?\n",
    "- Predict **categorical output**.\n",
    "- Input: Features \\(X\\)\n",
    "- Output: Class label \\(y\\)\n",
    "- Uses **labeled data** for training.\n",
    "- Can be:\n",
    "  - **Binary Classification** ‚Üí 0 or 1  \n",
    "  - **Multiclass Classification** ‚Üí 0, 1, 2, ‚Ä¶  \n",
    "  - **Multi-label Classification** ‚Üí multiple labels per instance  \n",
    "\n",
    "---\n",
    "\n",
    "# 2Ô∏è‚É£ Important Concepts in Classification\n",
    "\n",
    "## ‚úî Decision Boundary  \n",
    "The line/surface that separates classes.\n",
    "\n",
    "## ‚úî Probability Output  \n",
    "Many classifiers output probabilities, e.g., logistic regression, random forest, XGBoost.\n",
    "\n",
    "## ‚úî Threshold\n",
    "Default = 0.5  \n",
    "If \\( P(y=1) > 0.5 \\) ‚Üí class 1 else 0.\n",
    "\n",
    "## ‚úî Evaluation Metrics  \n",
    "- Accuracy  \n",
    "- Precision  \n",
    "- Recall  \n",
    "- F1-score  \n",
    "- Confusion Matrix  \n",
    "- ROC Curve & AUC  \n",
    "- Log Loss  \n",
    "\n",
    "## ‚úî Overfitting & Underfitting  \n",
    "- High variance ‚Üí overfit (e.g., deep trees)  \n",
    "- High bias ‚Üí underfit (e.g., linear models)  \n",
    "\n",
    "## ‚úî Regularization  \n",
    "Used to avoid overfitting  \n",
    "- L1 (Lasso)  \n",
    "- L2 (Ridge)  \n",
    "\n",
    "---\n",
    "\n",
    "# 3Ô∏è‚É£ Types of Classification Algorithms\n",
    "\n",
    "## üîπ A. Linear Models\n",
    "### **1. Logistic Regression**\n",
    "- Predicts probabilities using **sigmoid function**\n",
    "- Good for linearly separable data  \n",
    "- Fast, interpretable  \n",
    "\n",
    "---\n",
    "\n",
    "## üîπ B. Distance-Based Models\n",
    "### **2. K-Nearest Neighbors (KNN)**\n",
    "- No training needed (lazy learning)\n",
    "- Classifies using nearest neighbors  \n",
    "- Distance metrics: Euclidean, Manhattan  \n",
    "- Sensitive to scaling  \n",
    "\n",
    "---\n",
    "\n",
    "## üîπ C. Tree-Based Models\n",
    "### **3. Decision Tree**\n",
    "- Splits data using feature thresholds  \n",
    "- Easy to visualize  \n",
    "- Can overfit  \n",
    "\n",
    "### **4. Random Forest**\n",
    "- Ensemble of multiple decision trees  \n",
    "- Reduces overfitting  \n",
    "- Good accuracy  \n",
    "\n",
    "### **5. Extra Trees (ETC)**\n",
    "- Randomized tree splits  \n",
    "- Faster than Random Forest  \n",
    "\n",
    "### **6. Gradient Boosting**\n",
    "- Sequentially improves errors  \n",
    "- More powerful but slower  \n",
    "\n",
    "### **7. AdaBoost**\n",
    "- Boosts weak learners  \n",
    "- Adjusts weight of misclassified samples  \n",
    "\n",
    "### **8. XGBoost**\n",
    "- Most popular boosting algorithm  \n",
    "- Regularization + parallel processing  \n",
    "- Very high performance  \n",
    "\n",
    "### **9. LightGBM**\n",
    "- Faster than XGBoost  \n",
    "- Works well with large datasets  \n",
    "\n",
    "### **10. CatBoost**\n",
    "- Best for categorical features  \n",
    "- No need for One-Hot-Encoding  \n",
    "\n",
    "---\n",
    "\n",
    "## üîπ D. Support Vector Models\n",
    "### **11. Support Vector Machine (SVM)**\n",
    "- Finds the best separating **hyperplane**\n",
    "- Kernel tricks:\n",
    "  - Linear  \n",
    "  - RBF  \n",
    "  - Polynomial  \n",
    "  - Sigmoid  \n",
    "- Works well in high-dimensional data  \n",
    "\n",
    "---\n",
    "\n",
    "## üîπ E. Probabilistic Models\n",
    "### **12. Naive Bayes**\n",
    "- Based on Bayes Theorem  \n",
    "- Types:\n",
    "  - Gaussian NB  \n",
    "  - Multinomial NB (text data)  \n",
    "  - Bernoulli NB  \n",
    "- Very fast  \n",
    "- Used heavily in NLP  \n",
    "\n",
    "---\n",
    "\n",
    "## üîπ F. Neural Network Models\n",
    "### **13. Multi-Layer Perceptron (MLP)**\n",
    "- Feed-forward neural network  \n",
    "- Good for non-linear classification  \n",
    "\n",
    "### **14. Convolutional Neural Networks (CNN)**\n",
    "- Best for image classification  \n",
    "\n",
    "### **15. Recurrent Neural Networks (RNN, LSTM, GRU)**\n",
    "- Used for sequence data (text, speech)  \n",
    "\n",
    "---\n",
    "\n",
    "## üîπ G. Ensemble Methods (Mix of Models)\n",
    "### **16. Bagging Classifier**\n",
    "- Trains models on random subsets of data  \n",
    "\n",
    "### **17. Voting Classifier**\n",
    "- Majority voting from multiple models  \n",
    "\n",
    "### **18. Stacking Classifier**\n",
    "- Output of models ‚Üí final meta-model  \n",
    "\n",
    "---\n",
    "\n",
    "# 4Ô∏è‚É£ When to Use Which Algorithm?\n",
    "\n",
    "| Data Type | Best Models |\n",
    "|----------|-------------|\n",
    "| Linear | Logistic Regression, Linear SVM |\n",
    "| Small dataset | Naive Bayes, Decision Tree |\n",
    "| Large dataset | LightGBM, Random Forest |\n",
    "| Non-linear | SVM (RBF), Neural Networks |\n",
    "| High-dimensional | SVM, Naive Bayes |\n",
    "| Text classification | Multinomial NB, Logistic Regression |\n",
    "| Image | CNN |\n",
    "| Tabular Data | XGBoost, CatBoost, Random Forest |\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114f5cfe",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
